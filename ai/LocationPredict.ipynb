{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from scipy.spatial import distance\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastLSTM:\n",
    "    def __init__(self, random_seed: int = 1234):\n",
    "        self.random_seed = random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_dataset(self, df: pd.DataFrame) -> np.array:\n",
    "    if \"y\" in df.columns:\n",
    "        df = df.drop(columns=[\"y\"]).assign(y=df[\"y\"])\n",
    "    else:\n",
    "        raise KeyError(\"Not found 'y' in dataset\")\n",
    "    \n",
    "    dataset = df.values.reshape(df.shape)\n",
    "    return dataset\n",
    "\n",
    "ForecastLSTM.reshape_dataset = reshape_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(\n",
    "        self, dataset: np.array, seq_len: int, steps: int, single_output: bool\n",
    ") -> tuple:\n",
    "    \n",
    "    # feature와 y 각각 sequential dataset을 반환할 리스트\n",
    "    X, y = list(), list()\n",
    "    # sequence length와 step에 따라 생성\n",
    "    for i, _ in enumerate(dataset):\n",
    "        idx_in = i + seq_len\n",
    "        idx_out = idx_in + steps\n",
    "        if idx_out > len(dataset):\n",
    "            break\n",
    "        seq_x = dataset[i:idx_in, :-1]\n",
    "        if single_output:\n",
    "            seq_y = dataset[idx_out -1 : idx_out, -1]\n",
    "        else:\n",
    "            seq_y = dataset[idx_in:idx_out, -1]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "ForecastLSTM.split_sequences = split_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_dataset(\n",
    "        self, df: pd.DataFrame,\n",
    "        seq_len: int,\n",
    "        steps: int,\n",
    "        single_output: bool,\n",
    "        validation_split: float = 0.2,\n",
    "        verbose: bool = True\n",
    ") -> tuple:\n",
    "    # df -> np.array\n",
    "    dataset = self.reshape_dataset(df=df)\n",
    "\n",
    "    # feature, y를 sequential dataset으로 분리\n",
    "    X, y = self.split_sequences(\n",
    "        dataset=dataset,\n",
    "        seq_len=seq_len,\n",
    "        steps=steps,\n",
    "        single_output=single_output\n",
    "    )\n",
    "\n",
    "    # X, y에서 validation dataset 분리\n",
    "    dataset_size = len(X)\n",
    "    train_size = int(dataset_size * (1-validation_split))\n",
    "    X_train, y_train = X[:train_size, :], y[:train_size, :]\n",
    "    X_val, y_val = X[train_size:, :], y[train_size:, :]\n",
    "    if verbose:\n",
    "        print(f\" >>> X_train: {X_train.shape}\")\n",
    "        print(f\" >>> y_train: {y_train.shape}\")\n",
    "        print(f\" >>> X_val: {X_val.shape}\")\n",
    "        print(f\" >>> y_val: {y_val.shape}\")\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "ForecastLSTM.split_train_valid_dataset = split_train_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_lstm_model(\n",
    "    self,\n",
    "    seq_len: int,\n",
    "    n_features: int,\n",
    "    lstm_units: list,\n",
    "    learning_rate: float,\n",
    "    dropout: float,\n",
    "    steps: int,\n",
    "    metrics: list,\n",
    "    single_output: bool,\n",
    "    last_lstm_return_sequences: bool = False,\n",
    "    dense_units: list = None,\n",
    "    activation: str = None\n",
    "):\n",
    "    tf.random.set_seed(self.random_seed)\n",
    "    model = Sequential()\n",
    "\n",
    "    if len(lstm_units) > 1:\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                units=lstm_units[0],\n",
    "                activation=activation,\n",
    "                return_sequences=True,\n",
    "                input_shape=(seq_len, n_features)\n",
    "            )\n",
    "        )\n",
    "        lstm_layers = lstm_units[1:]\n",
    "        for i, n_units in enumerate(lstm_layers, start=1):\n",
    "            if i == len(lstm_layers):\n",
    "                if single_output:\n",
    "                    return_sequences = False\n",
    "                else:\n",
    "                    return_sequences = last_lstm_return_sequences\n",
    "                model.add(\n",
    "                    LSTM(\n",
    "                        units=n_units,\n",
    "                        activation=activation,\n",
    "                        return_sequences=return_sequences\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                model.add(\n",
    "                    LSTM(\n",
    "                        units=n_units,\n",
    "                        activation=activation,\n",
    "                        return_sequences=True\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        if single_output:\n",
    "            return_sequences = False\n",
    "        else:\n",
    "            return_sequences = last_lstm_return_sequences\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                units=lstm_units[0],\n",
    "                activation=activation,\n",
    "                return_sequences=return_sequences,\n",
    "                input_shape=(seq_len, n_features),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if single_output:\n",
    "        if dense_units:\n",
    "            for n_units in dense_units:\n",
    "                model.add(Dense(units=n_units, activation=activation))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(rate=dropout))\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        if last_lstm_return_sequences:\n",
    "            model.add(Flatten())\n",
    "        if dense_units:\n",
    "            for n_units in dense_units:\n",
    "                model.add(Dense(units=n_units, activation=activation))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(rate=dropout))\n",
    "        model.add(Dense(units=steps))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=MSE, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "ForecastLSTM.build_and_compile_lstm_model = build_and_compile_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(\n",
    "    self,\n",
    "    df: pd.DataFrame,\n",
    "    steps: int,\n",
    "    lstm_units: list,\n",
    "    activation: str,\n",
    "    dropout: float = 0,\n",
    "    seq_len: int = 10,\n",
    "    single_output: bool = False,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = None,\n",
    "    steps_per_epoch: int = None,\n",
    "    learning_rate: float = 0.001,\n",
    "    patience: int = 10,\n",
    "    validation_split: float = 0.2,\n",
    "    last_lstm_return_sequences: bool = False,\n",
    "    dense_units: list = None,\n",
    "    metrics: list = [\"mse\"],\n",
    "    check_point_path: str = None,\n",
    "    verbose: bool = False,\n",
    "    plot: bool = True\n",
    "):\n",
    "    \n",
    "    np.random.seed(self.random_seed)\n",
    "    tf.random.set_seed(self.random_seed)\n",
    "    (\n",
    "        self.X_train,\n",
    "        self.y_train,\n",
    "        self.X_val,\n",
    "        self.y_val\n",
    "    ) = self.split_train_valid_dataset(\n",
    "        df=df,\n",
    "        seq_len=seq_len,\n",
    "        steps=steps,\n",
    "        validation_split=validation_split,\n",
    "        single_output=single_output,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    n_features = df.shape[1] - 1\n",
    "    self.model = self.build_and_compile_lstm_model(\n",
    "        seq_len=seq_len,\n",
    "        n_features=n_features,\n",
    "        lstm_units=lstm_units,\n",
    "        activation=activation,\n",
    "        learning_rate=learning_rate,\n",
    "        dropout=dropout,\n",
    "        steps=steps,\n",
    "        last_lstm_return_sequences=last_lstm_return_sequences,\n",
    "        dense_units=dense_units,\n",
    "        metrics=metrics,\n",
    "        single_output=single_output,\n",
    "    )\n",
    "\n",
    "    # best model save\n",
    "    if check_point_path is not None:\n",
    "        checkpoint_path = f\"checkpoint/lstm_{check_point_path}.h5\"\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        rlr = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=patience, verbose=verbose\n",
    "        )\n",
    "        callbacks = [checkpoint, EarlyStopping(patience=patience), rlr]\n",
    "    else:\n",
    "        rlr = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=patience, verbose=verbose\n",
    "        )\n",
    "        callbacks = [EarlyStopping(patience=patience), rlr]\n",
    "\n",
    "    self.history = self.model.fit(\n",
    "        self.X_train,\n",
    "        self.y_train,\n",
    "        batch_size=batch_size,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=(self.X_val, self.y_val),\n",
    "        epochs=epochs,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    if check_point_path is not None: \n",
    "        self.model.load_weights(f\"checkpoint/lstm_{check_point_path}.h5\")\n",
    "\n",
    "    # 시각화\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.history.history[f\"{metrics[0]}\"])\n",
    "        plt.plot(self.history.history[f\"val_{metrics[0]}\"])\n",
    "        plt.title(\"Performance Metric\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(f\"{metrics[0]}\")\n",
    "        if metrics == \"mape\":\n",
    "            plt.axhline(y=10, xmin=0, xmax=1, color=\"grey\", ls=\"--\", alpha=0.5)\n",
    "        plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "ForecastLSTM.fit_lstm = fit_lstm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_validation_dataset(self) -> pd.DataFrame:\n",
    "    y_pred_list, y_val_list = list(), list()\n",
    "\n",
    "    for x_val, y_val in zip(self.X_val, self.y_val):\n",
    "        x_val = np.expand_dims(\n",
    "            x_val, axis=0\n",
    "        ) # (seq_len, n_features) -> (1, seq_len, n_features)\n",
    "        y_pred = self.model.predict(x_val)[0]\n",
    "        y_pred_list.extend(y_pred.tolist())\n",
    "        y_val_list.extend(y_val.tolist())\n",
    "    return pd.DataFrame({\"y\": y_val_list, \"yhat\": y_pred_list})\n",
    "\n",
    "\n",
    "ForecastLSTM.forecast_validation_dataset = forecast_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df_fcst: pd.DataFrame) -> dict:\n",
    "    true = df_fcst[\"y\"]\n",
    "    pred = df_fcst[\"yhat\"]\n",
    "\n",
    "    mae = (true - pred).abs().mean()\n",
    "    mape = (true - pred).abs().div(true).mean() * 100\n",
    "    mse = ((true - pred) ** 2).mean()\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape,\n",
    "        \"mse\": mse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
